# Evaluating Parsing Methods {#testing-parsing-methods-chapter}

Since the project is taking part in a retail context as mentioned in Chapter \@ref(aims) and that different techniques will be compared as mentioned in Chapter \@ref(query-parsing), being able to evaluate each technique is important. This chapter discusses the evaluation metrics that will be used in order to determine the better parsing method as well as the results of feeding the test dataset into the parsing methods.

## Evaluation Metrics {#eval-metrics}

### Accuracy {#accuracy}

This metric is simply a measure of how many SQL queries did the methods generate that match the actual solution. This metric however does have some issues of which will be discussed below.

#### Results Equivalency {#results-equivalency}

According to the ISO/IEC 9075:1992 which is the 1992 standard of SQL [@internationalstandardsorganisationISOIEC90751992] , under Chapter 13: **Data Manipulation** sub-section **\<declare cursor\>** it states under **General Rules**

> 2) If an \<order by clause\> is not specified, then the table specified by the \<cursor specification\> is T and the ordering of rows in T is implementation-dependent.

Since the queries above do not have an `ORDER BY` clause, the ordering of the rows in table \@ref(tab:query-results) is not guaranteed to be the same across different implementations of the SQL language by different relational databases (MySQL, PostgreSQL, SQLite are all examples of relational databases that implement SQL). For example, MySQL might return the items in table \@ref(tab:query-results) with `id=0` first then `id=1` where as SQLite might return `id=1` first then `id=0`. 

##### Solution

The solution to this is to ensure that each entry in the SQL table has a numerical unique identifier that way, the generated query and its results can be further sorted by its identifier in order to ensure that results of the query are equivalent regardless of database as the queries can be sorted and compared by its identifier.

Another solution is to run all the parsing methods in a single relational database that way, the ordering of the rows are consistent as row ordering is implementation specific. Since SQLite is already being used to store data related to the project (see Appendix \@ref(dataset-schema-explained)) SQLite is the database that all the parsing methods will use. 

SQLAlchemy, according to it [documentation](https://docs.sqlalchemy.org/en/13/dialects/sqlite.html#module-sqlalchemy.dialects.sqlite.pysqlite), uses `pysqlite` which *"is the same driver as the sqlite3 module included with the Python distribution."* In the interest of reproducibility, the sqlite3 driver version is described bellow as well as the python code used to obtain the version:

```{r,echo=FALSE}
library(reticulate)
```

```{python sqlite-version, attr.source='.numberLines', tidy=TRUE}
import sqlite3
print(sqlite3.sqlite_version)
```


#### SQL Query Equivalency {#query-equivalency}

There are some **SQL queries** that are syntactically different but when executed, return the same result. Table \@ref(tab:example-table) will be used to demonstrate this issue.

```{r example-table, echo=FALSE}
sql_table_issue = read.csv("tables/sql-table-issue.csv")
knitr::kable(
  head(sql_table_issue, 20), caption = 'An example SQL table called "test"',
  booktabs = TRUE, col.names = gsub("[.]", " ", names(sql_table_issue))
)
```

If we want to, for example, run a query that will *"select all the products that are in the category drinks"*, then the following SQL queries will return the same results. Each query also has a short description on what differentiates it from the other queries and is not an exhaustive list of all possible combinations. 

```{sql sql-similar, eval=FALSE, attr.source='.numberLines', tidy=TRUE, tidy.opts=list(width.cutoff=80)}
-- Query 1: Common SQL query
SELECT * FROM test WHERE category='drinks'
-- Query 2: Selecting table columns using table_name.column_name
SELECT test.id, test.product_name, test.size, test.category, test.brand FROM 
    test WHERE category='drinks'
-- Query 3: Selecting table columns using column_name only
SELECT id, product_name, size, category, brand FROM test WHERE category='drinks'
-- Query 4: Mixed use of lowercase and UPPERCASE SQL keywords
select id, product_name, size, category, brand FROM test 
    where test.category='drinks'
```
The results of all 4 queries above is as follows 

```{r db-open, include=FALSE}
library(DBI)
db = dbConnect(RSQLite::SQLite(), dbname = "sql-table-issue.sqlite")
```

```{sql db-query, connection=db, echo=FALSE, output.var="RESULTS"}
SELECT * FROM test WHERE category='drinks'
```

```{r query-results, echo=FALSE}
knitr::kable(
  head(RESULTS, 20), caption = "Results of the query", longtable = TRUE,
  booktabs = TRUE, col.names = gsub("[.]", " ", names(RESULTS))
)
```
\newpage

##### Solution {#query-equivalency-solution}

The solution is to determine if the queries are equivalent or not by looking at the results of the query. If they are equivalent, then they will return the same results. One way to do this is to execute both the author SQL query and the generated SQL query and see if the results match, if they do, then we can consider the two queries as equal and that the model has correctly generated the SQL query. 

### Ease of use {#ease-of-use}

Since the end user will receive the SQL query and (possibly), the results of the query executed, this section will discuss how *"easy"* it is for IT Infrastructure staff and/or Application developers to use the query parsing methods. Since this *"easy"* is not quantifiable, the following observations will be made and will be left up to the reader to decide. The observations are:

1. How easy it is to install
2. Resource requirements
3. Using the parsing method

## FerreroJeremy/ln2sql Evaluation {#ln2sql-eval}

### Accuracy {#ln2sql-eval-accuracy}

When ran against the test dataset as mentioned in Chapter \@ref(gathering-NLQ), we measured how many queries the method calculated (using the method described in Chapter \@ref(query-equivalency-solution)) correctly and how many incorrectly across both generic and explicit. The results are available in Figure \@ref(fig:ln2sql-graphs)

```{r ln2sql-graphs, echo=FALSE, fig.cap="results for ln2sql", out.width = "90%"}
knitr::include_graphics("results/ln2sql/ln2sql-results.png")
```

\newpage The graph on its own suggest that a natural language query that is more explicit is more likely to generate a query that returns the same result as the author query but not perfect whereas a generic query has approximately a 50% chance of calculating it correctly.

Looking at the results in `Rmarkdown Report/results/ln2sql/`, for explicit it managed to correctly generate 80% of the SQL queries. One particularly interesting query it got wrong was the `id = 4` in Table \@ref(tab:ln2sql-interesting-results-explicit). Even though the word *"or"* was explicitly mentioned, ln2sql generated an *"and"*. What is also interesting is that `id = 8` was able to correctly generate an *"or"* although the fact that a different column and different comparator was used. Running, (in interactive mode), `show me all the products where the price is 2 or the price is 1` gives `SELECT * FROM products WHERE products.price = '2' OR products.price = '1'` so given that the comparator is the same, we could infer that it is either the column that causes this deviation, or the fact that we are comapring intergers rather than strings which leads to the model suggesting an *"or"*.

```{r ln2sql-interesting-results-explicit, echo=FALSE, warning=FALSE}
sql_table_issue = read.csv("results/ln2sql/results_of_interest_explicit.csv")
knitr::kable(
  head(sql_table_issue, 20), caption = 'queries of interest in explicit',
  booktabs = TRUE, col.names = gsub("[.]", " ", names(sql_table_issue))
)%>% kable_styling(full_width = T)  
```
\newpage For generic, the model performed much more poorly (Table \@ref(tab:ln2sql-interesting-results-generic)). It got more than half wrong. `id 1 - 3` suggests that, in comparison to the explicit dataset, having the value first and column name second will result in an incorrect prediction where as having it the other way around (as in the explicit dataset) will result in the correct prediction. This is where the model mainly fails in the generic dataset. The model can handle simple queries such `id = 9 ,11 - 13` but apart from that, ln2sql is more suited to queries that follow the structure of a SQL query.   

```{r ln2sql-interesting-results-generic, echo=FALSE, warning=FALSE}
sql_table_issue = read.csv("results/ln2sql/results_of_interest_generic.csv")
knitr::kable(
  head(sql_table_issue, 20), caption = 'queries of interest in generic',
  booktabs = TRUE, col.names = gsub("[.]", " ", names(sql_table_issue))
)%>% kable_styling(full_width = T)  
```

### Ease of Use {#ln2sql-eval-ease-of-use}

1. **How easy is it to install**

Installing ln2sql is very easy as the repository has very easy to follow instructions and has most of the resources needed in the repository. While the original [@ferreroFerreroJeremyLn2sql2020] has a very simple user interface as well as an Command Line interface (CLI) through the terminal, it is very easy use the same code that powers the CLI in your own code. This paper, for example used the code to make a terminal interface with colours as well as the ability to execute and return the queries. The code for that is available here [@patelZayd62Ln2sql2020]

2. **Resource requirements**

ln2sql has no external dependencies and a significant slow down was not experienced. The only externel dependency was **pytest** a python testing framework [@PytestdevPytest2020] so as long as you are able to run python (tested on Python 3.7), you should be able to clone the repository and start parsing natural language queries  

3. **Using the parsing method**

The parsing method is just a simple as having an SQL dump file which contains the structure of the databse as well as a language configuration file (included with the repository) and feeding the program an english natural language. Keyword filtering can be improved through the use of a thesaurus (included) as well as the ability to improve the stop word filtering as well. A CLI tool and a GUI tool is included as well. 

**Conclusion**

Overall, ln2sql is a very quick and easy way to start parsing SQL queries as it is simple to install, run wherever python can run and is open to improvement due to available source code, paper and various configuration files all available to tweak. One small caveat is that the built in GUI and CLI requires the `-m` flag (`python -m ...`) which according to the python CLI docs (accessed via `python -h` on Ubuntu 18.04 terminal), run library module as a script so you would probably have to convert the module to a python package which is beyond the scope of this paper.

## paulfitz/mlsql

### Accuracy {#sqlova-accuracy}

Just like Chapter \@ref(ln2sql-eval-accuracy), we measured the accuracy between the explicit and generic dataset and measured how many were correct or incorrect. The results are in \@ref(fig:results-sqlova)

```{r results-sqlova, echo=FALSE, fig.cap="Results for mlsql", out.width = "80%"}
knitr::include_graphics("results/sqlova/sqlova-results.png")
```

\newpage Data suggests that sqlova performed worse than ln2sql based on the graph alone. However, taking a look at the data itself for generic Table \@ref(tab:sqlova-results-generic), `id 1 to 3` appear to have the *where* filter correct, the *select* filter is *itemname* rather than *"\*"*.  Another reason is that sqlova is based on a deep learning approach so to get the best results, the model will need to be trained on some of samples of data relevant to the retail domain where as sqlova was trained on Chapter \@ref(wikisql).

```{r sqlova-results-generic, echo=FALSE, warning=FALSE}
sql_table_issue = read.csv("results/sqlova/test_results_sqlova_generic.csv")
knitr::kable(
  head(sql_table_issue, 5), caption = 'first 5 tests in generic',
  booktabs = TRUE, col.names = gsub("[.]", " ", names(sql_table_issue))
)%>% kable_styling(full_width = T)  
```

### Ease of Use {#sqlova-eval-ease-of-use}

1. **How easy is it to install**

Installation was very simple as the model was made available via a Docker Container so once the contaier was downloaded, only once command is need to run the models. One small issue is that the download was over 4GB so for slow internet conections, unreliable connections and meterred connections may not be able to use this. 

2. **Resource requirements**

One benefit of Docker Containers is that no dependency management is needed but the Docker Software itself is needed to run the container. A strong internet connection is needed to download the model as it is over 4GB and it requires 3GB of RAM which is quite when ran on consumer hardware.  

3. **Using the parsing method**

The model is ONLY interacted with through a REST API so can be accessed programmatically. One way that this can be used by end users is by placing a user interface, e.g. Desktop of Web Application which then communicates with the API on the users behalf. Also, given the fact that this is a Deep learning approach, debugging and improving the will require specialist talent. Deploying the container designed to be simple due to the containarised nature. 

**Conclusion**

While the REST API is nice from a developers perspective, given the weaker performance and higher memory usage, this approach is not as nad as ln2sql. 