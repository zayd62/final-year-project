# Data Gathering {#data-gathering-chapter}

As mentioned in Chapter \@ref(data-gathering), a test dataset is required in order to evaluate the different parsing methods as discussed in Chapter \@ref(query-parsing-chapter). 

```{bash generate-dataset-ERD-plantuml, include=FALSE}
plantuml figures/plantuml/dataset-ERD.puml
```

## Gathering the test Data {#gathering-data}

As discussed is Chapter \@ref(data-gathering), the data we will be gathering is **product data**. In order to obtain this data, we will be web scraping. The website we will be using is [https://www.bestwaywholesale.co.uk/](https://www.bestwaywholesale.co.uk) [@bestwaywholesaleUKCashCarry] and we will use **scrapy**, a python library, to perform the web scraping [@scrapyScrapyScrapy2019] and BeautifulSoup4, another python library, [@richardsonBeautifulSoupWe] for parsing the HTML.


### Scraping {#scraping}
 
As mentioned in Chapter \@ref(gathering-data), we use scrapy, a python package to iterate through webpages, extracting the information required and storing it in a database. The database we are using is SQLite [@sqliteteamSQLiteHomePage] and SQLAlchemy [@sqlalchemySqlalchemySqlalchemy2019] is used as the ORM (see Chapter \@ref(definitions) for more information on ORMs).

Using [@krebsSQLAlchemyORMTutorial] as guidance on how to use SQLAlchemy, A database schema was designed and is visualised in figure \@ref(fig:dataset-schema-diagram) as an ERD. \newpage

```{r dataset-schema-diagram, echo=FALSE, fig.cap="Entity Relationship Diagram for the dataset"}
knitr::include_graphics("figures/plantuml/dataset-ERD.png")
```

The entities mentioned in figure \@ref(fig:dataset-schema-diagram) are explained in Appendix \@ref(dataset-schema-explained).

> **Note**: Not all the entities in the diagram above are used in the dataset scraping. The entities used in the dataset scraping are found in Appendix \@ref(tables-raw-scrapped-data).

### Scraping Process {#scraping-process}

The scraping process involves various files. Below is a description of each file used in the scraping process and their function. The files are given as a path from the root of the GitHub repository that the files are stored in [@patelZayd62Finalyearproject2019] (Note that the files below are **directly** involved in the scraping process).

- **table_definitions/base.py**: Used for defining the database.
- **table_definitions/category.py**: A class used to create rows in the category table.
- **table_definitions/page.py**: A class used to create rows in the page table.
- **table_definitions/product.py**: A class used to create rows in the product table.
- **table_definitions/productdata.py**: A class used to create rows in the productdata table.
- **html_parse/page_product.py**: A function which given a category, gets the associated page rows. For each HTMLcontent in page row, find all the products elements and store in the database.
- **html_parse/productdata.py**: A function which converts a HTML table to a dictionary. Used to extract detailed information about a scraped product. Used by **scrapers.py** (specifically `class crawlProduct(Spider)` found in **scrapers.py**).
- **database.py**: Used to create the actual database file. Uses **table_definitions/base.py** to help create the database.
- **scrapers.py**: The code responsible for visiting the web pages and getting the HTML that is going to be parsed.
- **raw_data_pipeline.py**: The "glue" code that takes all the files mentioned above and performs a complete scrape from a category to scraped products.

The scraped data is available in the file **dataset.sqlite3** in this reports associated GitHub repository [@patelZayd62Finalyearproject2019]. **Note**:  Some of the data has been manually cleaned.

## Gathering Natural Language Queries {#gathering-NLQ}

In order to test the various methods, a set of natural language queries as well as their corresponding SQL queries will be needed. The questions will be stored in `dataset.sqlite3` in the tables `authorQuestions` and `alternativeQuestions` details of which can be found in Appendix \@ref(question-tables)

### Questions {#nlq-questions}

One of the aims of the reports is to find out how domain general model performs in a domain specific environment. One way to test this is to have a natural language query which is then mapped to an SQL query; this will be stored in a table called `authorQuestions`. In order to be more thorough in testing, a set of *"alternative questions"* will be written. These are natural language queries that when parsed, should produce the same SQL query as other natural language queries. The reason for doing this is to test the robustness of the domain general models as well as attempting to *simulate* how these models will be used in a production environment as a service like this is likely to be used by different people and different people phrase things differently but yet can mean the same thing when mapped to SQL. This will be stored in a table `alternativeQuestions`.


## Dependencies {#ch4-dependencies}
A file called `requirements-dataGathering.txt` contains a list of all the python packages used for data